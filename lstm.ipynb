{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n"
      ],
      "metadata": {
        "id": "mvalbenTRjvj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lstm(nn.Module):\n",
        "    def __init__(self, hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        mean = torch.tensor(0.0)\n",
        "        std = torch.tensor(0.1)\n",
        "\n",
        "        self.wlr1 = nn.Parameter(torch.normal(mean, std, size=(hidden_size, hidden_size)))\n",
        "        self.wlr2 = nn.Parameter(torch.normal(mean, std, size=(1, hidden_size)))\n",
        "        self.blr1 = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.wpr1 = nn.Parameter(torch.normal(mean, std, size=(hidden_size, hidden_size)))\n",
        "        self.wpr2 = nn.Parameter(torch.normal(mean, std, size=(1, hidden_size)))\n",
        "        self.bpr1 = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.wp1 = nn.Parameter(torch.normal(mean, std, size=(hidden_size, hidden_size)))\n",
        "        self.wp2 = nn.Parameter(torch.normal(mean, std, size=(1, hidden_size)))\n",
        "        self.bp1 = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.wo1 = nn.Parameter(torch.normal(mean, std, size=(hidden_size, hidden_size)))\n",
        "        self.wo2 = nn.Parameter(torch.normal(mean, std, size=(1, hidden_size)))\n",
        "        self.bo1 = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def lstm_unit(self, input_value, long_term_state, short_term_state):\n",
        "        input_value = input_value.view(1, -1)\n",
        "\n",
        "        long_remember_percent = torch.sigmoid(torch.mm(short_term_state, self.wlr1) +\n",
        "                                           torch.mm(input_value, self.wlr2) + self.blr1)\n",
        "\n",
        "        potential_remember_percent = torch.sigmoid(torch.mm(short_term_state, self.wpr1) +\n",
        "                                                torch.mm(input_value, self.wpr2) + self.bpr1)\n",
        "\n",
        "        potential_memory = torch.tanh(torch.mm(short_term_state, self.wp1) +\n",
        "                                   torch.mm(input_value, self.wp2) + self.bp1)\n",
        "\n",
        "        updated_long_term_state = (long_remember_percent * long_term_state +\n",
        "                                potential_remember_percent * potential_memory)\n",
        "\n",
        "        output_percent = torch.sigmoid(torch.mm(short_term_state, self.wo1) +\n",
        "                                    torch.mm(input_value, self.wo2) + self.bo1)\n",
        "\n",
        "        updated_short_term_state = output_percent * torch.tanh(updated_long_term_state)\n",
        "\n",
        "        return updated_long_term_state, updated_short_term_state\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        batch_size = 1\n",
        "        long_term_state = torch.zeros(batch_size, self.hidden_size)\n",
        "        short_term_state = torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "        for i in range(len(input_seq)):\n",
        "            long_term_state, short_term_state = self.lstm_unit(\n",
        "                input_seq[i],\n",
        "                long_term_state,\n",
        "                short_term_state\n",
        "            )\n",
        "\n",
        "        output = self.output_layer(short_term_state)\n",
        "        return output.squeeze()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_i, label_i = batch\n",
        "        output_i = self(input_i)\n",
        "        loss = F.mse_loss(output_i, label_i)  # MSE loss\n",
        "        return loss"
      ],
      "metadata": {
        "id": "UzXtVugcGmQx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lstm_improved():\n",
        "    sequences = torch.tensor([\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.2, 0.4, 0.6, 0.8, 1.0],\n",
        "        [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "        [0.05, 0.1, 0.15, 0.2, 0.25]\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    targets = torch.tensor([0.6, 1.2, 1.1, 0.3], dtype=torch.float32)\n",
        "\n",
        "    dataset = TensorDataset(sequences, targets)\n",
        "    train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    model = Lstm(hidden_size=32)\n",
        "    optimizer = model.configure_optimizers()\n",
        "\n",
        "    num_epochs = 1000\n",
        "    best_loss = float('inf')\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_sequences, batch_targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = 0\n",
        "            for seq, target in zip(batch_sequences, batch_targets):\n",
        "                output = model(seq)\n",
        "                batch_loss += F.mse_loss(output, target)\n",
        "\n",
        "            batch_loss /= len(batch_sequences)\n",
        "            batch_loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_epoch_loss:.6f}')\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_sequences = torch.tensor([\n",
        "            [0.15, 0.3, 0.45, 0.6, 0.75],\n",
        "            [0.2, 0.25, 0.3, 0.35, 0.4],\n",
        "            [0.1, 0.2, 0.4, 0.8, 1.6]\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        expected_values = torch.tensor([0.9, 0.45, 3.2], dtype=torch.float32)\n",
        "\n",
        "        for i, (test_seq, expected) in enumerate(zip(test_sequences, expected_values)):\n",
        "            prediction = model(test_seq)\n",
        "            print(f'\\nTest Sequence {i+1}: {test_seq.tolist()}')\n",
        "            print(f'Predicted value: {prediction.item():.4f}')\n",
        "            print(f'Expected value: {expected.item():.4f}')\n",
        "            print(f'Prediction error: {abs(prediction.item() - expected.item()):.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_lstm_improved()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAJptPeXc44",
        "outputId": "d2ef9773-66af-4964-c427-2786fb6c11e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [10/1000], Average Loss: 0.434728\n",
            "Epoch [20/1000], Average Loss: 0.258023\n",
            "Epoch [30/1000], Average Loss: 0.053162\n",
            "Epoch [40/1000], Average Loss: 0.048572\n",
            "Epoch [50/1000], Average Loss: 0.040620\n",
            "Epoch [60/1000], Average Loss: 0.031184\n",
            "Epoch [70/1000], Average Loss: 0.024332\n",
            "Epoch [80/1000], Average Loss: 0.018692\n",
            "Epoch [90/1000], Average Loss: 0.013903\n",
            "Epoch [100/1000], Average Loss: 0.010252\n",
            "Epoch [110/1000], Average Loss: 0.007994\n",
            "Epoch [120/1000], Average Loss: 0.006605\n",
            "Epoch [130/1000], Average Loss: 0.005768\n",
            "Epoch [140/1000], Average Loss: 0.005579\n",
            "Epoch [150/1000], Average Loss: 0.005263\n",
            "Epoch [160/1000], Average Loss: 0.005284\n",
            "Epoch [170/1000], Average Loss: 0.004952\n",
            "Epoch [180/1000], Average Loss: 0.004818\n",
            "Epoch [190/1000], Average Loss: 0.004931\n",
            "Epoch [200/1000], Average Loss: 0.004753\n",
            "Epoch [210/1000], Average Loss: 0.004517\n",
            "Epoch [220/1000], Average Loss: 0.004381\n",
            "Epoch [230/1000], Average Loss: 0.004276\n",
            "Epoch [240/1000], Average Loss: 0.004311\n",
            "Epoch [250/1000], Average Loss: 0.004163\n",
            "Epoch [260/1000], Average Loss: 0.003960\n",
            "Epoch [270/1000], Average Loss: 0.004087\n",
            "Epoch [280/1000], Average Loss: 0.003743\n",
            "Epoch [290/1000], Average Loss: 0.003635\n",
            "Epoch [300/1000], Average Loss: 0.003606\n",
            "Epoch [310/1000], Average Loss: 0.003620\n",
            "Epoch [320/1000], Average Loss: 0.003295\n",
            "Epoch [330/1000], Average Loss: 0.003436\n",
            "Epoch [340/1000], Average Loss: 0.003167\n",
            "Epoch [350/1000], Average Loss: 0.003063\n",
            "Epoch [360/1000], Average Loss: 0.002862\n",
            "Epoch [370/1000], Average Loss: 0.002815\n",
            "Epoch [380/1000], Average Loss: 0.002783\n",
            "Epoch [390/1000], Average Loss: 0.002492\n",
            "Epoch [400/1000], Average Loss: 0.002565\n",
            "Epoch [410/1000], Average Loss: 0.002392\n",
            "Epoch [420/1000], Average Loss: 0.002279\n",
            "Epoch [430/1000], Average Loss: 0.001977\n",
            "Epoch [440/1000], Average Loss: 0.001924\n",
            "Epoch [450/1000], Average Loss: 0.001761\n",
            "Epoch [460/1000], Average Loss: 0.001681\n",
            "Epoch [470/1000], Average Loss: 0.001765\n",
            "Epoch [480/1000], Average Loss: 0.001460\n",
            "Epoch [490/1000], Average Loss: 0.001334\n",
            "Epoch [500/1000], Average Loss: 0.001220\n",
            "Epoch [510/1000], Average Loss: 0.001271\n",
            "Epoch [520/1000], Average Loss: 0.001008\n",
            "Epoch [530/1000], Average Loss: 0.000955\n",
            "Epoch [540/1000], Average Loss: 0.000906\n",
            "Epoch [550/1000], Average Loss: 0.000743\n",
            "Epoch [560/1000], Average Loss: 0.000660\n",
            "Epoch [570/1000], Average Loss: 0.000635\n",
            "Epoch [580/1000], Average Loss: 0.000523\n",
            "Epoch [590/1000], Average Loss: 0.000501\n",
            "Epoch [600/1000], Average Loss: 0.000416\n",
            "Epoch [610/1000], Average Loss: 0.000369\n",
            "Epoch [620/1000], Average Loss: 0.000350\n",
            "Epoch [630/1000], Average Loss: 0.000347\n",
            "Epoch [640/1000], Average Loss: 0.000270\n",
            "Epoch [650/1000], Average Loss: 0.000241\n",
            "Epoch [660/1000], Average Loss: 0.000221\n",
            "Epoch [670/1000], Average Loss: 0.000197\n",
            "Epoch [680/1000], Average Loss: 0.000207\n",
            "Epoch [690/1000], Average Loss: 0.000171\n",
            "Epoch [700/1000], Average Loss: 0.000164\n",
            "Epoch [710/1000], Average Loss: 0.000154\n",
            "Epoch [720/1000], Average Loss: 0.000157\n",
            "Epoch [730/1000], Average Loss: 0.000139\n",
            "Epoch [740/1000], Average Loss: 0.000148\n",
            "Epoch [750/1000], Average Loss: 0.000142\n",
            "Epoch [760/1000], Average Loss: 0.000127\n",
            "Epoch [770/1000], Average Loss: 0.000125\n",
            "Epoch [780/1000], Average Loss: 0.000127\n",
            "Epoch [790/1000], Average Loss: 0.000117\n",
            "Epoch [800/1000], Average Loss: 0.000122\n",
            "Epoch [810/1000], Average Loss: 0.000117\n",
            "Epoch [820/1000], Average Loss: 0.000113\n",
            "Epoch [830/1000], Average Loss: 0.000096\n",
            "Epoch [840/1000], Average Loss: 0.000092\n",
            "Epoch [850/1000], Average Loss: 0.000088\n",
            "Epoch [860/1000], Average Loss: 0.000098\n",
            "Epoch [870/1000], Average Loss: 0.000093\n",
            "Epoch [880/1000], Average Loss: 0.000090\n",
            "Epoch [890/1000], Average Loss: 0.000081\n",
            "Epoch [900/1000], Average Loss: 0.000080\n",
            "Epoch [910/1000], Average Loss: 0.000077\n",
            "Epoch [920/1000], Average Loss: 0.000073\n",
            "Epoch [930/1000], Average Loss: 0.000069\n",
            "Epoch [940/1000], Average Loss: 0.000068\n",
            "Epoch [950/1000], Average Loss: 0.000066\n",
            "Epoch [960/1000], Average Loss: 0.000070\n",
            "Epoch [970/1000], Average Loss: 0.000068\n",
            "Epoch [980/1000], Average Loss: 0.000060\n",
            "Epoch [990/1000], Average Loss: 0.000057\n",
            "Epoch [1000/1000], Average Loss: 0.000054\n",
            "\n",
            "Testing the model...\n",
            "\n",
            "Test Sequence 1: [0.15000000596046448, 0.30000001192092896, 0.44999998807907104, 0.6000000238418579, 0.75]\n",
            "Predicted value: 0.8943\n",
            "Expected value: 0.9000\n",
            "Prediction error: 0.0057\n",
            "\n",
            "Test Sequence 2: [0.20000000298023224, 0.25, 0.30000001192092896, 0.3499999940395355, 0.4000000059604645]\n",
            "Predicted value: 0.4523\n",
            "Expected value: 0.4500\n",
            "Prediction error: 0.0023\n",
            "\n",
            "Test Sequence 3: [0.10000000149011612, 0.20000000298023224, 0.4000000059604645, 0.800000011920929, 1.600000023841858]\n",
            "Predicted value: 1.4405\n",
            "Expected value: 3.2000\n",
            "Prediction error: 1.7595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGmxcDCIYRJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}